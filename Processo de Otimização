#!/usr/bin/env python
# coding: utf-8

# In[162]:


# Importando Bibliotecas Necessárias
import numpy as np
import math
import matplotlib.pyplot as plt
from scipy.optimize import minimize 
import numdifftools as nd
import statistics
from numdifftools import Jacobian, Hessian
import warnings


# In[163]:


with warnings.catch_warnings():
    warnings.simplefilter("error")


# In[164]:


# Define a Variância, x[0], x[1] e x[2] representam as variáveis k, alpha e beta, respectivamente.

def er(x):
    
    n = np.size(o)
    soma = sum(((o[: n] - x[0]*( (np.e)**(x[1]*( d[: n]**x[2] ) )) )**2)/n)
            
    return ((np.float64(soma)))


# In[165]:


# Dado que inicial de que deve ser oferecido ao método
x0 = np.array([0.5, 0.5, 0.5], dtype = np.float64)

# Dados de f(x) e x, exemplares, que serão modelados pela função densidade de probabilidade . f(x) = o e x = d.
o = np.array([np.e, (np.e)**2, (np.e)**3, (np.e)**4, (np.e)**5, (np.e)**6], dtype = np.float64)
d = np.array([1, 2, 3, 4, 5, 6], dtype = np.float64)


# In[166]:


# Define o Jacobiano
def jacobi(x):
    return Jacobian(lambda x: er(x))(x).ravel()


# In[167]:


# Define o Hessiano
def hessi(x):
    return Hessian(lambda x: er(x))(x)


# In[168]:


# Método Trust Region Krylov
res = minimize(er, x0, method='trust-krylov',
               jac=jacobi, hess = hessi,
               options={'gtol': 1e-5, 'disp': True})


# In[169]:


# Retorna os valores [x[0], x[1], x[2]] = [k, alpha, beta ] para que o erro seja mínimo, ou seja Currrent Fuction Value 
# no ponto mínimo.
res.x
